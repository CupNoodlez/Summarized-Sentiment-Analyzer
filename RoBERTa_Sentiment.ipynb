{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b15d91df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': 1,\n",
       " 'message': 'The sea floor is sinking under the weight of climate change https://t.co/R9Uhnjfg7G',\n",
       " 'tweetid': 954625951685578752}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from CSV and prepare splits\n",
    "from datasets import load_dataset\n",
    "\n",
    "csv_path = r\"c:\\Users\\codes\\Documents\\Programming\\Summarized-Sentiment-Analyzer\\twitter_sentiment_data.csv\"\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": csv_path}, split=\"train\")\n",
    "\n",
    "# If your CSV has columns like 'text' and 'label' adjust here\n",
    "text_column = \"text\"\n",
    "label_column = \"label\"\n",
    "\n",
    "# Create validation split (10%)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds = dataset[\"train\"]\n",
    "val_ds = dataset[\"test\"]\n",
    "\n",
    "# Inspect a sample\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e63be568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.3.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\codes\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (2.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\codes\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\codes\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\codes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\codes\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install training dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install -U datasets scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f1c9095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install required packages (uncomment if needed)\n",
    "## Note: Running installs from the notebook may require internet access.\n",
    "## On Windows cmd, you can also install via terminal:\n",
    "## pip install --upgrade pip\n",
    "## pip install transformers torch sentencepiece\n",
    "\n",
    "# If you prefer inline install, uncomment the following:\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install -U pip\n",
    "# !{sys.executable} -m pip install transformers torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "62c51ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix notebook progress bars and optional HF Xet support\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade ipywidgets jupyter jupyterlab notebook\n",
    "# Optional: speed up Hugging Face downloads\n",
    "# !{sys.executable} -m pip install \"huggingface_hub[hf_xet]\"\n",
    "# Optional: silence symlink warning without enabling Developer Mode\n",
    "# import os\n",
    "# os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "edc03e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'negative', 1: 'neutral', 2: 'positive'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize RoBERTa sentiment pipeline using CardiffNLP model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Build a TextClassificationPipeline\n",
    "sentiment_pipeline = TextClassificationPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    framework=\"pt\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "labels = model.config.id2label\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0d88f70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love this product! -> positive (0.985)\n",
      "This is the worst experience ever. -> negative (0.945)\n",
      "It's okay, nothing special. -> neutral (0.599)\n",
      "I hate you so much! -> negative (0.935)\n",
      "I adore you so much! -> positive (0.984)\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is the worst experience ever.\",\n",
    "    \"It's okay, nothing special.\",\n",
    "    \"I hate you so much!\",\n",
    "    \"I adore you so much!\"\n",
    "]\n",
    "\n",
    "results = sentiment_pipeline(texts, top_k=1)\n",
    "for text, res in zip(texts, results):\n",
    "    print(f\"{text} -> {res[0]['label']} ({res[0]['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9de6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 1: Data Loading & Preparation\n",
    "\n",
    "Load the labeled dataset with all required features: text, sentiment_label, timestamp, platform, and geolocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6ad0cd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (43943, 3)\n",
      "Columns: ['sentiment', 'message', 'tweetid']\n",
      "\n",
      "First few rows:\n",
      "   sentiment                                            message  \\\n",
      "0         -1  @tiniebeany climate change is an interesting h...   \n",
      "1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
      "2          1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
      "3          1  RT @Mick_Fanning: Just watched this amazing do...   \n",
      "4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
      "\n",
      "              tweetid  \n",
      "0  792927353886371840  \n",
      "1  793124211518832641  \n",
      "2  793124402388832256  \n",
      "3  793124635873275904  \n",
      "4  793125156185137153  \n",
      "\n",
      "Using 'sentiment' as sentiment column\n",
      "Using 'message' as text column\n",
      "\n",
      "Original Sentiment Distribution:\n",
      "sentiment_label\n",
      " 1    22962\n",
      " 2     9276\n",
      " 0     7715\n",
      "-1     3990\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Detected numeric sentiments. Converting to strings...\n",
      "\n",
      "After conversion:\n",
      "sentiment_label\n",
      "Positive    32238\n",
      "Neutral      7715\n",
      "Negative     3990\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Warning: No 'timestamp' column found. Creating dummy timestamps.\n",
      "Warning: No 'platform' column found. Creating default.\n",
      "Warning: No 'geolocation' column found. Creating default.\n",
      "\n",
      "âœ“ Data loading complete. Ready for label encoding.\n",
      "Warning: No 'platform' column found. Creating default.\n",
      "Warning: No 'geolocation' column found. Creating default.\n",
      "\n",
      "âœ“ Data loading complete. Ready for label encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\codes\\AppData\\Local\\Temp\\ipykernel_7120\\3856150145.py:72: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df['timestamp'] = pd.date_range(start='2024-01-01', periods=len(df), freq='H')\n"
     ]
    }
   ],
   "source": [
    "# Load labeled dataset with all features\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load your pre-processed and labeled data\n",
    "csv_path = r\"twitter_sentiment_data.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Print actual columns to diagnose issues\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Auto-detect sentiment column name (flexible column naming)\n",
    "sentiment_col = None\n",
    "for possible_name in ['sentiment_label', 'sentiment', 'label', 'Sentiment', 'target']:\n",
    "    if possible_name in df.columns:\n",
    "        sentiment_col = possible_name\n",
    "        break\n",
    "\n",
    "if sentiment_col is None:\n",
    "    raise ValueError(f\"Could not find sentiment column. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\nUsing '{sentiment_col}' as sentiment column\")\n",
    "\n",
    "# Auto-detect text column name\n",
    "text_col = None\n",
    "for possible_name in ['text', 'message', 'tweet', 'content', 'Message']:\n",
    "    if possible_name in df.columns:\n",
    "        text_col = possible_name\n",
    "        break\n",
    "\n",
    "if text_col is None:\n",
    "    raise ValueError(f\"Could not find text column. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "print(f\"Using '{text_col}' as text column\")\n",
    "\n",
    "# Standardize column names\n",
    "if sentiment_col != 'sentiment_label':\n",
    "    df['sentiment_label'] = df[sentiment_col]\n",
    "\n",
    "if text_col != 'text':\n",
    "    df['text'] = df[text_col]\n",
    "\n",
    "# Check label distribution BEFORE conversion\n",
    "print(f\"\\nOriginal Sentiment Distribution:\")\n",
    "print(df['sentiment_label'].value_counts())\n",
    "\n",
    "# Convert numeric sentiments to strings if needed\n",
    "# Common mappings: -1=Negative, 0=Neutral, 1=Positive, 2=Positive\n",
    "if df['sentiment_label'].dtype in ['int64', 'float64']:\n",
    "    print(\"\\nDetected numeric sentiments. Converting to strings...\")\n",
    "    numeric_to_string = {\n",
    "        -1: 'Negative',\n",
    "        0: 'Neutral',\n",
    "        1: 'Positive',\n",
    "        2: 'Positive'  # Sometimes datasets use 2 for positive\n",
    "    }\n",
    "    df['sentiment_label'] = df['sentiment_label'].map(numeric_to_string)\n",
    "    \n",
    "    print(f\"\\nAfter conversion:\")\n",
    "    print(df['sentiment_label'].value_counts())\n",
    "\n",
    "# Convert timestamps to datetime if timestamp column exists\n",
    "if 'timestamp' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    print(f\"\\nDate Range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "else:\n",
    "    print(\"\\nWarning: No 'timestamp' column found. Creating dummy timestamps.\")\n",
    "    df['timestamp'] = pd.date_range(start='2024-01-01', periods=len(df), freq='H')\n",
    "\n",
    "# Ensure other columns exist or create defaults\n",
    "if 'platform' not in df.columns:\n",
    "    print(\"Warning: No 'platform' column found. Creating default.\")\n",
    "    df['platform'] = 'Twitter'\n",
    "\n",
    "if 'geolocation' not in df.columns:\n",
    "    print(\"Warning: No 'geolocation' column found. Creating default.\")\n",
    "    df['geolocation'] = 'Unknown'\n",
    "\n",
    "print(f\"\\nâœ“ Data loading complete. Ready for label encoding.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0074a06",
   "metadata": {},
   "source": [
    "## Phase 2: Label Encoding & Dataset Preparation\n",
    "\n",
    "Map sentiment labels to numeric IDs and prepare train/validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4c81a6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing labels after mapping: 0\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "0     3990\n",
      "1     7715\n",
      "2    32238\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Split sizes:\n",
      "Train: 30760\n",
      "Validation: 6591\n",
      "Test: 6592\n",
      "\n",
      "âœ“ Datasets created successfully\n",
      "\n",
      "âœ“ Datasets created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create label mapping: Positive=2, Neutral=1, Negative=0\n",
    "label_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "id2label = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Encode labels (sentiment_label column now exists from previous cell)\n",
    "df['label'] = df['sentiment_label'].map(label_map)\n",
    "\n",
    "# Verify no missing labels after mapping\n",
    "missing_count = df['label'].isna().sum()\n",
    "print(f\"Missing labels after mapping: {missing_count}\")\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"\\nWarning: Found {missing_count} unmapped labels. These will be removed.\")\n",
    "    print(f\"Unmapped values: {df[df['label'].isna()]['sentiment_label'].unique()}\")\n",
    "    df = df.dropna(subset=['label'])\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts().sort_index())\n",
    "\n",
    "# Create train/validation/test splits (70/15/15)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"Train: {len(train_df)}\")\n",
    "print(f\"Validation: {len(val_df)}\")\n",
    "print(f\"Test: {len(test_df)}\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'label']].reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_df[['text', 'label']].reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df[['text', 'label', 'timestamp', 'platform', 'geolocation']].reset_index(drop=True))\n",
    "\n",
    "print(f\"\\nâœ“ Datasets created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f98b5e",
   "metadata": {},
   "source": [
    "## Phase 3: Model Fine-Tuning Setup\n",
    "\n",
    "Configure RoBERTa tokenizer and prepare datasets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9f35d218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7ba6ac062545a5b26216e301ff0843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8a9501d53e45c3b9a1b71ddacfe507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6591 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b3d7cafaf44158933b0f4ee01d8824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6592 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: CPU\n",
      "Tokenized train samples: 30760\n"
     ]
    }
   ],
   "source": [
    "# Initialize RoBERTa tokenizer and tokenize datasets\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=512, padding=False)\n",
    "\n",
    "# Tokenize all datasets\n",
    "train_tokenized = train_dataset.map(preprocess_function, batched=True)\n",
    "val_tokenized = val_dataset.map(preprocess_function, batched=True)\n",
    "test_tokenized = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"Tokenized train samples: {len(train_tokenized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f196385e",
   "metadata": {},
   "source": [
    "## Phase 4: Model Fine-Tuning & Training\n",
    "\n",
    "Configure model architecture, training arguments, and execute fine-tuning with comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9121ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install training dependencies (accelerate required for Trainer)\n",
    "# IMPORTANT: After running this cell, you may need to restart the kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q accelerate>=0.26.0\n",
    "\n",
    "# Force reload transformers to pick up newly installed accelerate\n",
    "import importlib\n",
    "import transformers\n",
    "importlib.reload(transformers)\n",
    "\n",
    "print(\"âœ“ Accelerate installed. If you still see errors, restart the kernel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "de25d8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[112]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     25\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: accuracy_score(labels, predictions),\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1_macro\u001b[39m\u001b[33m\"\u001b[39m: f1_score(labels, predictions, average=\u001b[33m\"\u001b[39m\u001b[33mmacro\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     27\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1_weighted\u001b[39m\u001b[33m\"\u001b[39m: f1_score(labels, predictions, average=\u001b[33m\"\u001b[39m\u001b[33mweighted\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     28\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1_per_class\u001b[39m\u001b[33m\"\u001b[39m: f1_score(labels, predictions, average=\u001b[38;5;28;01mNone\u001b[39;00m).tolist()\n\u001b[32m     29\u001b[39m     }\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Training configuration optimized for sentiment analysis\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./models/roberta-environmental-sentiment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mf1_macro\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Mixed precision if GPU available\u001b[39;49;00m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable wandb/tensorboard for now\u001b[39;49;00m\n\u001b[32m     50\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Initialize Trainer with early stopping\u001b[39;00m\n\u001b[32m     53\u001b[39m trainer = Trainer(\n\u001b[32m     54\u001b[39m     model=model,\n\u001b[32m     55\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m     callbacks=[EarlyStoppingCallback(early_stopping_patience=\u001b[32m2\u001b[39m)]\n\u001b[32m     62\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:135\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\codes\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1811\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1809\u001b[39m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[32m   1810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[32m-> \u001b[39m\u001b[32m1811\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.torchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1814\u001b[39m     warnings.warn(\n\u001b[32m   1815\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`torchdynamo` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1816\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `torch_compile_backend` instead\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1817\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m   1818\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\codes\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:2355\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2351\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2352\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   2353\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2354\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_devices\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\codes\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\functools.py:1026\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m   1024\u001b[39m val = cache.get(\u001b[38;5;28mself\u001b[39m.attrname, _NOT_FOUND)\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1027\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1028\u001b[39m         cache[\u001b[38;5;28mself\u001b[39m.attrname] = val\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\codes\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:2225\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m2225\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   2226\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2227\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2228\u001b[39m         )\n\u001b[32m   2229\u001b[39m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[32m   2230\u001b[39m accelerator_state_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33muse_configured_state\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "# Configure RoBERTa model and training pipeline\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# Initialize model with label mappings\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label_map\n",
    ")\n",
    "\n",
    "# Define comprehensive evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1_macro\": f1_score(labels, predictions, average=\"macro\"),\n",
    "        \"f1_weighted\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "        \"f1_per_class\": f1_score(labels, predictions, average=None).tolist()\n",
    "    }\n",
    "\n",
    "# Training configuration optimized for sentiment analysis\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/roberta-environmental-sentiment\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision if GPU available\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard for now\n",
    ")\n",
    "\n",
    "# Initialize Trainer with early stopping\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"Starting model fine-tuning...\")\n",
    "print(f\"Training samples: {len(train_tokenized)}\")\n",
    "print(f\"Validation samples: {len(val_tokenized)}\")\n",
    "print(f\"Device: {training_args.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e87b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute training\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_metrics = trainer.evaluate()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VALIDATION METRICS\")\n",
    "print(\"=\"*50)\n",
    "for key, value in val_metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Save the best model\n",
    "model_save_path = \"./models/roberta-environmental-sentiment-best\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"\\nâœ“ Model saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c87319",
   "metadata": {},
   "source": [
    "## Phase 5: Model Evaluation on Test Set\n",
    "\n",
    "Generate predictions and detailed performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e4cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "predictions = trainer.predict(test_tokenized)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "pred_probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=-1).numpy()\n",
    "\n",
    "# Get true labels\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Compute test metrics\n",
    "test_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "test_f1_macro = f1_score(true_labels, pred_labels, average=\"macro\")\n",
    "test_f1_weighted = f1_score(true_labels, pred_labels, average=\"weighted\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"TEST SET METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"F1 Macro: {test_f1_macro:.4f}\")\n",
    "print(f\"F1 Weighted: {test_f1_weighted:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(\n",
    "    true_labels, \n",
    "    pred_labels, \n",
    "    target_names=[\"Negative\", \"Neutral\", \"Positive\"],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\"*50)\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(\"           Predicted\")\n",
    "print(\"           Neg  Neu  Pos\")\n",
    "for i, label in enumerate([\"Negative\", \"Neutral\", \"Positive\"]):\n",
    "    print(f\"Actual {label:8s} {cm[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e883c",
   "metadata": {},
   "source": [
    "## Phase 6: Prepare Predictions DataFrame\n",
    "\n",
    "Create enriched dataset with predictions, probabilities, and metadata for temporal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results dataframe\n",
    "results_df = test_df.copy()\n",
    "results_df['predicted_label'] = pred_labels\n",
    "results_df['predicted_sentiment'] = results_df['predicted_label'].map(id2label)\n",
    "results_df['prob_negative'] = pred_probs[:, 0]\n",
    "results_df['prob_neutral'] = pred_probs[:, 1]\n",
    "results_df['prob_positive'] = pred_probs[:, 2]\n",
    "results_df['confidence'] = pred_probs.max(axis=1)\n",
    "\n",
    "# Calculate sentiment score (-1 to +1 scale)\n",
    "results_df['sentiment_score'] = (\n",
    "    results_df['prob_positive'] - results_df['prob_negative']\n",
    ")\n",
    "\n",
    "# Ensure timestamp is datetime\n",
    "results_df['timestamp'] = pd.to_datetime(results_df['timestamp'])\n",
    "\n",
    "print(f\"Results DataFrame shape: {results_df.shape}\")\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(results_df[['text', 'sentiment_label', 'predicted_sentiment', 'sentiment_score', 'confidence']].head(10))\n",
    "\n",
    "# Save predictions for future analysis\n",
    "results_df.to_csv(\"predictions_with_metadata.csv\", index=False)\n",
    "print(\"\\nâœ“ Predictions saved to: predictions_with_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb0af82",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 7: Temporal Aggregation & Trend Analysis\n",
    "\n",
    "Aggregate sentiment scores by time windows and platform to identify patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cbfb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal aggregation functions\n",
    "def aggregate_by_time(df, freq='W', score_col='sentiment_score'):\n",
    "    \"\"\"\n",
    "    Aggregate sentiment by time period\n",
    "    freq: 'D' (daily), 'W' (weekly), 'M' (monthly)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.set_index('timestamp')\n",
    "    \n",
    "    aggregated = df.groupby(pd.Grouper(freq=freq)).agg({\n",
    "        score_col: ['mean', 'std', 'count'],\n",
    "        'prob_positive': 'mean',\n",
    "        'prob_neutral': 'mean',\n",
    "        'prob_negative': 'mean',\n",
    "        'confidence': 'mean'\n",
    "    })\n",
    "    \n",
    "    aggregated.columns = ['_'.join(col).strip() for col in aggregated.columns.values]\n",
    "    aggregated = aggregated.reset_index()\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "# Weekly aggregation\n",
    "weekly_sentiment = aggregate_by_time(results_df, freq='W')\n",
    "print(\"Weekly Sentiment Aggregation:\")\n",
    "print(weekly_sentiment.head(10))\n",
    "print(f\"\\nTotal weeks: {len(weekly_sentiment)}\")\n",
    "\n",
    "# Monthly aggregation\n",
    "monthly_sentiment = aggregate_by_time(results_df, freq='M')\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Monthly Sentiment Aggregation:\")\n",
    "print(monthly_sentiment.head())\n",
    "\n",
    "# Aggregate by platform\n",
    "platform_sentiment = results_df.groupby('platform').agg({\n",
    "    'sentiment_score': ['mean', 'std', 'count'],\n",
    "    'prob_positive': 'mean',\n",
    "    'prob_neutral': 'mean',\n",
    "    'prob_negative': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "platform_sentiment.columns = ['_'.join(col).strip() if col[1] else col[0] \n",
    "                               for col in platform_sentiment.columns.values]\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Platform-wise Sentiment:\")\n",
    "print(platform_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebdb9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Platform aggregation for cross-analysis\n",
    "weekly_platform = results_df.set_index('timestamp').groupby([\n",
    "    pd.Grouper(freq='W'),\n",
    "    'platform'\n",
    "]).agg({\n",
    "    'sentiment_score': 'mean',\n",
    "    'text': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "weekly_platform.columns = ['timestamp', 'platform', 'avg_sentiment', 'post_count']\n",
    "\n",
    "print(\"Weekly Sentiment by Platform:\")\n",
    "print(weekly_platform.head(15))\n",
    "\n",
    "# Calculate rolling averages (4-week moving average)\n",
    "results_df_sorted = results_df.sort_values('timestamp')\n",
    "results_df_sorted = results_df_sorted.set_index('timestamp')\n",
    "\n",
    "rolling_sentiment = results_df_sorted['sentiment_score'].resample('D').mean().rolling(\n",
    "    window=28, min_periods=7\n",
    ").mean().reset_index()\n",
    "\n",
    "rolling_sentiment.columns = ['timestamp', 'rolling_avg_28d']\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"28-Day Rolling Average Sentiment:\")\n",
    "print(rolling_sentiment.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1090b8fd",
   "metadata": {},
   "source": [
    "## Phase 8: Time-Series Visualization\n",
    "\n",
    "Create comprehensive time-series charts showing sentiment evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overall sentiment trends over time\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 1. Weekly Sentiment Time Series with all sentiment types\n",
    "fig1 = go.Figure()\n",
    "\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=weekly_sentiment['timestamp'],\n",
    "    y=weekly_sentiment['prob_positive_mean'],\n",
    "    mode='lines+markers',\n",
    "    name='Positive',\n",
    "    line=dict(color='green', width=2),\n",
    "    marker=dict(size=6)\n",
    "))\n",
    "\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=weekly_sentiment['timestamp'],\n",
    "    y=weekly_sentiment['prob_neutral_mean'],\n",
    "    mode='lines+markers',\n",
    "    name='Neutral',\n",
    "    line=dict(color='gray', width=2),\n",
    "    marker=dict(size=6)\n",
    "))\n",
    "\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=weekly_sentiment['timestamp'],\n",
    "    y=weekly_sentiment['prob_negative_mean'],\n",
    "    mode='lines+markers',\n",
    "    name='Negative',\n",
    "    line=dict(color='red', width=2),\n",
    "    marker=dict(size=6)\n",
    "))\n",
    "\n",
    "fig1.update_layout(\n",
    "    title='Weekly Sentiment Distribution: Environmental Policy Perception',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Average Probability',\n",
    "    hovermode='x unified',\n",
    "    template='plotly_white',\n",
    "    height=500,\n",
    "    legend=dict(x=0.01, y=0.99)\n",
    ")\n",
    "\n",
    "fig1.show()\n",
    "\n",
    "# 2. Sentiment Score Time Series (Composite -1 to +1)\n",
    "fig2 = go.Figure()\n",
    "\n",
    "fig2.add_trace(go.Scatter(\n",
    "    x=weekly_sentiment['timestamp'],\n",
    "    y=weekly_sentiment['sentiment_score_mean'],\n",
    "    mode='lines+markers',\n",
    "    name='Sentiment Score',\n",
    "    line=dict(color='blue', width=3),\n",
    "    marker=dict(size=8),\n",
    "    fill='tozeroy',\n",
    "    fillcolor='rgba(0,100,255,0.2)'\n",
    "))\n",
    "\n",
    "# Add zero line for reference\n",
    "fig2.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", opacity=0.5)\n",
    "\n",
    "fig2.update_layout(\n",
    "    title='Weekly Sentiment Score Trend (-1: Negative, +1: Positive)',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Average Sentiment Score',\n",
    "    hovermode='x unified',\n",
    "    template='plotly_white',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Platform Comparison Time Series\n",
    "fig3 = px.line(\n",
    "    weekly_platform,\n",
    "    x='timestamp',\n",
    "    y='avg_sentiment',\n",
    "    color='platform',\n",
    "    markers=True,\n",
    "    title='Sentiment Trends by Platform',\n",
    "    labels={'avg_sentiment': 'Average Sentiment Score', 'timestamp': 'Date'}\n",
    ")\n",
    "\n",
    "fig3.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", opacity=0.3)\n",
    "\n",
    "fig3.update_layout(\n",
    "    hovermode='x unified',\n",
    "    template='plotly_white',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig3.show()\n",
    "\n",
    "# 4. Volume and Sentiment Combined View\n",
    "fig4 = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=('Sentiment Score Over Time', 'Post Volume Over Time'),\n",
    "    vertical_spacing=0.12,\n",
    "    specs=[[{\"secondary_y\": False}], [{\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# Sentiment trend\n",
    "fig4.add_trace(\n",
    "    go.Scatter(\n",
    "        x=weekly_sentiment['timestamp'],\n",
    "        y=weekly_sentiment['sentiment_score_mean'],\n",
    "        mode='lines+markers',\n",
    "        name='Sentiment Score',\n",
    "        line=dict(color='blue', width=2)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Volume trend\n",
    "fig4.add_trace(\n",
    "    go.Bar(\n",
    "        x=weekly_sentiment['timestamp'],\n",
    "        y=weekly_sentiment['sentiment_score_count'],\n",
    "        name='Post Count',\n",
    "        marker_color='lightblue'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig4.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "fig4.update_yaxes(title_text=\"Sentiment Score\", row=1, col=1)\n",
    "fig4.update_yaxes(title_text=\"Number of Posts\", row=2, col=1)\n",
    "\n",
    "fig4.update_layout(\n",
    "    title_text='Sentiment & Volume Analysis',\n",
    "    showlegend=True,\n",
    "    template='plotly_white',\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8007789",
   "metadata": {},
   "source": [
    "## Phase 9: Geographic Heatmap\n",
    "\n",
    "Visualize sentiment intensity by geolocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81996488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate sentiment by geolocation\n",
    "geo_sentiment = results_df.groupby('geolocation').agg({\n",
    "    'sentiment_score': ['mean', 'std', 'count'],\n",
    "    'prob_positive': 'mean',\n",
    "    'prob_negative': 'mean',\n",
    "    'prob_neutral': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "geo_sentiment.columns = ['geolocation', 'avg_sentiment', 'std_sentiment', \n",
    "                         'post_count', 'avg_positive', 'avg_negative', 'avg_neutral']\n",
    "\n",
    "# Sort by average sentiment\n",
    "geo_sentiment_sorted = geo_sentiment.sort_values('avg_sentiment', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Positive Regions:\")\n",
    "print(geo_sentiment_sorted.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Top 10 Most Negative Regions:\")\n",
    "print(geo_sentiment_sorted.tail(10))\n",
    "\n",
    "# Geographic Heatmap (Bar Chart representation)\n",
    "fig5 = px.bar(\n",
    "    geo_sentiment_sorted.head(20),\n",
    "    x='geolocation',\n",
    "    y='avg_sentiment',\n",
    "    color='avg_sentiment',\n",
    "    color_continuous_scale=['red', 'yellow', 'green'],\n",
    "    color_continuous_midpoint=0,\n",
    "    title='Top 20 Locations by Average Sentiment Score',\n",
    "    labels={'avg_sentiment': 'Average Sentiment', 'geolocation': 'Location'},\n",
    "    hover_data=['post_count', 'avg_positive', 'avg_negative']\n",
    ")\n",
    "\n",
    "fig5.update_layout(\n",
    "    xaxis_tickangle=-45,\n",
    "    template='plotly_white',\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig5.show()\n",
    "\n",
    "# Alternative: Heatmap Matrix by Location and Time Period\n",
    "# Prepare data for time-location heatmap\n",
    "results_df['month'] = results_df['timestamp'].dt.to_period('M').astype(str)\n",
    "\n",
    "geo_time_matrix = results_df.groupby(['geolocation', 'month'])['sentiment_score'].mean().reset_index()\n",
    "geo_time_pivot = geo_time_matrix.pivot(index='geolocation', columns='month', values='sentiment_score')\n",
    "\n",
    "# Filter to top locations by volume\n",
    "top_locations = results_df['geolocation'].value_counts().head(15).index\n",
    "geo_time_filtered = geo_time_pivot.loc[geo_time_pivot.index.isin(top_locations)]\n",
    "\n",
    "fig6 = px.imshow(\n",
    "    geo_time_filtered,\n",
    "    color_continuous_scale='RdYlGn',\n",
    "    color_continuous_midpoint=0,\n",
    "    title='Sentiment Heatmap: Top Locations Over Time',\n",
    "    labels=dict(x=\"Month\", y=\"Location\", color=\"Sentiment Score\"),\n",
    "    aspect='auto'\n",
    ")\n",
    "\n",
    "fig6.update_layout(\n",
    "    height=600,\n",
    "    xaxis_tickangle=-45\n",
    ")\n",
    "\n",
    "fig6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dddb11",
   "metadata": {},
   "source": [
    "## Phase 10: Predictive Forecasting\n",
    "\n",
    "Use Prophet for time-series forecasting of future sentiment trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Prophet forecasting\n",
    "from prophet import Prophet\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Daily sentiment for better forecasting\n",
    "daily_sentiment = results_df.set_index('timestamp').resample('D')['sentiment_score'].mean().reset_index()\n",
    "daily_sentiment.columns = ['ds', 'y']  # Prophet requires 'ds' and 'y' columns\n",
    "\n",
    "# Remove any NaN values\n",
    "daily_sentiment = daily_sentiment.dropna()\n",
    "\n",
    "print(f\"Training forecast model on {len(daily_sentiment)} days of data\")\n",
    "print(f\"Date range: {daily_sentiment['ds'].min()} to {daily_sentiment['ds'].max()}\")\n",
    "\n",
    "# Initialize and fit Prophet model\n",
    "model = Prophet(\n",
    "    daily_seasonality=False,\n",
    "    weekly_seasonality=True,\n",
    "    yearly_seasonality=True,\n",
    "    changepoint_prior_scale=0.05,  # Flexibility of trend changes\n",
    "    seasonality_prior_scale=10.0\n",
    ")\n",
    "\n",
    "model.fit(daily_sentiment)\n",
    "\n",
    "# Create future dataframe for 90-day forecast\n",
    "future = model.make_future_dataframe(periods=90)\n",
    "forecast = model.predict(future)\n",
    "\n",
    "print(f\"\\nâœ“ Forecast generated for {len(future)} total days (including historical + 90 future days)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baedcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forecast with Plotly\n",
    "fig7 = go.Figure()\n",
    "\n",
    "# Historical data\n",
    "fig7.add_trace(go.Scatter(\n",
    "    x=daily_sentiment['ds'],\n",
    "    y=daily_sentiment['y'],\n",
    "    mode='markers',\n",
    "    name='Historical Data',\n",
    "    marker=dict(size=4, color='blue', opacity=0.6)\n",
    "))\n",
    "\n",
    "# Forecast\n",
    "fig7.add_trace(go.Scatter(\n",
    "    x=forecast['ds'],\n",
    "    y=forecast['yhat'],\n",
    "    mode='lines',\n",
    "    name='Forecast',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "# Confidence interval\n",
    "fig7.add_trace(go.Scatter(\n",
    "    x=forecast['ds'],\n",
    "    y=forecast['yhat_upper'],\n",
    "    mode='lines',\n",
    "    name='Upper Bound',\n",
    "    line=dict(width=0),\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "fig7.add_trace(go.Scatter(\n",
    "    x=forecast['ds'],\n",
    "    y=forecast['yhat_lower'],\n",
    "    mode='lines',\n",
    "    name='Confidence Interval',\n",
    "    line=dict(width=0),\n",
    "    fillcolor='rgba(255, 0, 0, 0.1)',\n",
    "    fill='tonexty'\n",
    "))\n",
    "\n",
    "fig7.update_layout(\n",
    "    title='90-Day Sentiment Forecast: Environmental Policy Perception',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Sentiment Score',\n",
    "    hovermode='x unified',\n",
    "    template='plotly_white',\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig7.show()\n",
    "\n",
    "# Display forecast statistics\n",
    "future_only = forecast[forecast['ds'] > daily_sentiment['ds'].max()]\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FORECAST SUMMARY (Next 90 Days)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Average Predicted Sentiment: {future_only['yhat'].mean():.4f}\")\n",
    "print(f\"Trend Direction: {'Positive' if future_only['yhat'].iloc[-1] > future_only['yhat'].iloc[0] else 'Negative'}\")\n",
    "print(f\"Predicted Range: {future_only['yhat'].min():.4f} to {future_only['yhat'].max():.4f}\")\n",
    "print(f\"\\nLast Historical Sentiment: {daily_sentiment['y'].iloc[-1]:.4f}\")\n",
    "print(f\"First Forecast (tomorrow): {future_only['yhat'].iloc[0]:.4f}\")\n",
    "print(f\"Final Forecast (90 days): {future_only['yhat'].iloc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ece27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose forecast components (trend, weekly, yearly seasonality)\n",
    "from prophet.plot import plot_components_plotly\n",
    "\n",
    "fig_components = plot_components_plotly(model, forecast)\n",
    "fig_components.update_layout(height=800)\n",
    "fig_components.show()\n",
    "\n",
    "# Save forecast results\n",
    "forecast_export = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].copy()\n",
    "forecast_export.columns = ['date', 'predicted_sentiment', 'lower_bound', 'upper_bound']\n",
    "forecast_export.to_csv('sentiment_forecast_90days.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ“ Forecast saved to: sentiment_forecast_90days.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfee615",
   "metadata": {},
   "source": [
    "## Phase 11: Advanced Analytics & Insights\n",
    "\n",
    "Statistical analysis and key findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ef616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate trend statistics and volatility\n",
    "from scipy import stats\n",
    "\n",
    "# Overall sentiment statistics\n",
    "print(\"=\"*60)\n",
    "print(\"OVERALL SENTIMENT STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean Sentiment Score: {results_df['sentiment_score'].mean():.4f}\")\n",
    "print(f\"Median Sentiment Score: {results_df['sentiment_score'].median():.4f}\")\n",
    "print(f\"Std Dev: {results_df['sentiment_score'].std():.4f}\")\n",
    "print(f\"Skewness: {results_df['sentiment_score'].skew():.4f}\")\n",
    "print(f\"Kurtosis: {results_df['sentiment_score'].kurtosis():.4f}\")\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiment_dist = results_df['predicted_sentiment'].value_counts(normalize=True) * 100\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENTIMENT DISTRIBUTION (%)\")\n",
    "print(\"=\"*60)\n",
    "for sentiment, pct in sentiment_dist.items():\n",
    "    print(f\"{sentiment}: {pct:.2f}%\")\n",
    "\n",
    "# Trend analysis (linear regression on time)\n",
    "results_df['days_since_start'] = (results_df['timestamp'] - results_df['timestamp'].min()).dt.days\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "    results_df['days_since_start'], \n",
    "    results_df['sentiment_score']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEMPORAL TREND ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Trend Slope: {slope:.6f} per day\")\n",
    "print(f\"R-squared: {r_value**2:.4f}\")\n",
    "print(f\"P-value: {p_value:.4e}\")\n",
    "print(f\"Trend: {'Significantly ' if p_value < 0.05 else ''}{'Positive' if slope > 0 else 'Negative'}\")\n",
    "\n",
    "# Platform insights\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PLATFORM INSIGHTS\")\n",
    "print(\"=\"*60)\n",
    "for platform in platform_sentiment['platform']:\n",
    "    platform_data = platform_sentiment[platform_sentiment['platform'] == platform]\n",
    "    print(f\"\\n{platform}:\")\n",
    "    print(f\"  Avg Sentiment: {platform_data['sentiment_score_mean'].values[0]:.4f}\")\n",
    "    print(f\"  Total Posts: {int(platform_data['sentiment_score_count'].values[0])}\")\n",
    "    print(f\"  Positive Rate: {platform_data['prob_positive_mean'].values[0]:.2%}\")\n",
    "    print(f\"  Negative Rate: {platform_data['prob_negative_mean'].values[0]:.2%}\")\n",
    "\n",
    "# Identify sentiment shifts (significant changes week-over-week)\n",
    "weekly_sentiment['sentiment_change'] = weekly_sentiment['sentiment_score_mean'].diff()\n",
    "weekly_sentiment['pct_change'] = weekly_sentiment['sentiment_score_mean'].pct_change() * 100\n",
    "\n",
    "significant_shifts = weekly_sentiment[abs(weekly_sentiment['sentiment_change']) > 0.1]\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"SIGNIFICANT SENTIMENT SHIFTS (>0.1 change)\")\n",
    "print(\"=\"*60)\n",
    "if len(significant_shifts) > 0:\n",
    "    for _, row in significant_shifts.head(10).iterrows():\n",
    "        print(f\"{row['timestamp'].date()}: {row['sentiment_change']:+.3f} ({row['pct_change']:+.1f}%)\")\n",
    "else:\n",
    "    print(\"No significant shifts detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941dd666",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: MLOps Pipeline Complete âœ“\n",
    "\n",
    "**Completed Tasks:**\n",
    "\n",
    "1. âœ“ **Fine-Tuning**: RoBERTa-base fine-tuned on environmental policy sentiment with robust evaluation (F1, Accuracy)\n",
    "2. âœ“ **Model Evaluation**: Comprehensive metrics including confusion matrix and per-class F1 scores\n",
    "3. âœ“ **Prediction Pipeline**: Generated sentiment scores with confidence metrics for entire test set\n",
    "4. âœ“ **Temporal Aggregation**: Weekly/Monthly rolling averages grouped by time and platform\n",
    "5. âœ“ **Visualization**: Interactive Plotly charts showing sentiment evolution, platform comparison, and volume trends\n",
    "6. âœ“ **Geographic Analysis**: Heatmap showing sentiment intensity by location\n",
    "7. âœ“ **Forecasting**: 90-day Prophet forecast with confidence intervals and trend decomposition\n",
    "\n",
    "**Output Files:**\n",
    "- `models/roberta-environmental-sentiment-best/` - Fine-tuned model checkpoint\n",
    "- `predictions_with_metadata.csv` - Full predictions with timestamps, platforms, geolocation\n",
    "- `sentiment_forecast_90days.csv` - 90-day forecast data\n",
    "\n",
    "**Key Insights Available:**\n",
    "- Temporal trends and seasonality patterns\n",
    "- Platform-specific sentiment differences\n",
    "- Geographic sentiment distribution\n",
    "- Predictive outlook for next 90 days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb69d2f",
   "metadata": {},
   "source": [
    "# Predictive Analytics: Environmental Policy Sentiment Analysis\n",
    "\n",
    "**Study Title:** Forecasting Public Perception of Environmental Policy\n",
    "\n",
    "**Objective:** Fine-tune RoBERTa on labeled sentiment data, aggregate temporal trends, and forecast future sentiment patterns.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Model Fine-Tuning (RoBERTa)\n",
    "2. Inference & Prediction\n",
    "3. Temporal Aggregation\n",
    "4. Visualization & Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e6978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q transformers datasets torch scikit-learn pandas plotly prophet kaleido"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
