{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Streamlined Sentiment Analysis & Summary\n",
    "This notebook focuses on sentiment analysis and summary generation using a pre-trained RoBERTa model.\n",
    "No training required - uses cardiffnlp/twitter-roberta-base-sentiment-latest model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9j0k1l2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3n4o5p6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pre-trained RoBERTa sentiment model\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "print(f\"Loading pre-trained model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create sentiment pipeline\n",
    "sentiment_pipeline = TextClassificationPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    framework=\"pt\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Display label mappings\n",
    "labels = model.config.id2label\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"Label mappings: {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t0",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1v2w3x4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data from CSV\n",
    "# Adjust the path and column names as needed\n",
    "csv_path = \"1k_sample.csv\"  # Change to your file: twitter_sentiment_data.csv, 5k_sample.csv, etc.\n",
    "text_column = \"text\"  # Adjust column name if different\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Loaded {len(df)} records from {csv_path}\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y5z6a7b8",
   "metadata": {},
   "source": [
    "## 3. Perform Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few samples first\n",
    "sample_texts = df[text_column].head(5).tolist()\n",
    "print(\"Testing on sample texts:\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    result = sentiment_pipeline(text, top_k=1)\n",
    "    print(f\"{i}. Text: {text[:80]}...\")\n",
    "    print(f\"   Sentiment: {result[0]['label']} (confidence: {result[0]['score']:.3f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g3h4i5j6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all texts in batches for efficiency\n",
    "print(\"Analyzing all texts...\")\n",
    "\n",
    "batch_size = 32\n",
    "all_texts = df[text_column].fillna(\"\").tolist()\n",
    "all_results = []\n",
    "\n",
    "for i in range(0, len(all_texts), batch_size):\n",
    "    batch = all_texts[i:i+batch_size]\n",
    "    results = sentiment_pipeline(batch, top_k=1, truncation=True, max_length=512)\n",
    "    all_results.extend(results)\n",
    "    if (i // batch_size + 1) % 10 == 0:\n",
    "        print(f\"Processed {i+len(batch)}/{len(all_texts)} texts...\")\n",
    "\n",
    "print(f\"\\nAnalysis complete! Processed {len(all_results)} texts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k7l8m9n0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to dataframe\n",
    "df['predicted_sentiment'] = [result[0]['label'] for result in all_results]\n",
    "df['confidence_score'] = [result[0]['score'] for result in all_results]\n",
    "\n",
    "print(\"Predictions added to dataframe!\")\n",
    "print(f\"\\nSample results:\")\n",
    "df[[text_column, 'predicted_sentiment', 'confidence_score']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o1p2q3r4",
   "metadata": {},
   "source": [
    "## 4. Generate Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s5t6u7v8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall sentiment distribution\n",
    "sentiment_counts = df['predicted_sentiment'].value_counts()\n",
    "sentiment_percentages = df['predicted_sentiment'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SENTIMENT ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal texts analyzed: {len(df)}\")\n",
    "print(f\"\\n{'Sentiment':<15} {'Count':<10} {'Percentage':<10}\")\n",
    "print(\"-\" * 40)\n",
    "for sentiment in sentiment_counts.index:\n",
    "    count = sentiment_counts[sentiment]\n",
    "    pct = sentiment_percentages[sentiment]\n",
    "    print(f\"{sentiment:<15} {count:<10} {pct:>6.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w9x0y1z2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence statistics\n",
    "print(\"\\nCONFIDENCE SCORE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean confidence: {df['confidence_score'].mean():.4f}\")\n",
    "print(f\"Median confidence: {df['confidence_score'].median():.4f}\")\n",
    "print(f\"Min confidence: {df['confidence_score'].min():.4f}\")\n",
    "print(f\"Max confidence: {df['confidence_score'].max():.4f}\")\n",
    "print(f\"Std deviation: {df['confidence_score'].std():.4f}\")\n",
    "\n",
    "# Confidence by sentiment\n",
    "print(\"\\nAverage confidence by sentiment:\")\n",
    "for sentiment in df['predicted_sentiment'].unique():\n",
    "    avg_conf = df[df['predicted_sentiment'] == sentiment]['confidence_score'].mean()\n",
    "    print(f\"  {sentiment}: {avg_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8g9h0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Sentiment distribution (bar chart)\n",
    "sentiment_counts.plot(kind='bar', ax=axes[0], color=['#d62728', '#7f7f7f', '#2ca02c'])\n",
    "axes[0].set_title('Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentiment', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, v in enumerate(sentiment_counts):\n",
    "    axes[0].text(i, v + 0.5, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Sentiment distribution (pie chart)\n",
    "colors = ['#d62728', '#7f7f7f', '#2ca02c']\n",
    "sentiment_order = ['negative', 'neutral', 'positive']\n",
    "plot_data = [sentiment_counts.get(s, 0) for s in sentiment_order]\n",
    "axes[1].pie(plot_data, labels=sentiment_order, autopct='%1.1f%%', \n",
    "            startangle=90, colors=colors, textprops={'fontsize': 11})\n",
    "axes[1].set_title('Sentiment Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i1j2k3l4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confidence score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Overall confidence distribution\n",
    "axes[0].hist(df['confidence_score'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Confidence Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Confidence Score', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].axvline(df['confidence_score'].mean(), color='red', linestyle='--', \n",
    "                linewidth=2, label=f\"Mean: {df['confidence_score'].mean():.3f}\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Confidence by sentiment (box plot)\n",
    "df.boxplot(column='confidence_score', by='predicted_sentiment', ax=axes[1])\n",
    "axes[1].set_title('Confidence Score by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Sentiment', fontsize=12)\n",
    "axes[1].set_ylabel('Confidence Score', fontsize=12)\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5n6o7p8",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q9r0s1t2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show most confident predictions for each sentiment\n",
    "print(\"MOST CONFIDENT PREDICTIONS BY SENTIMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for sentiment in ['positive', 'neutral', 'negative']:\n",
    "    print(f\"\\n{sentiment.upper()}:\")\n",
    "    print(\"-\" * 80)\n",
    "    subset = df[df['predicted_sentiment'] == sentiment].nlargest(3, 'confidence_score')\n",
    "    for idx, (_, row) in enumerate(subset.iterrows(), 1):\n",
    "        text = row[text_column][:100] + \"...\" if len(row[text_column]) > 100 else row[text_column]\n",
    "        print(f\"{idx}. {text}\")\n",
    "        print(f\"   Confidence: {row['confidence_score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u3v4w5x6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show least confident predictions (potential edge cases)\n",
    "print(\"\\nLEAST CONFIDENT PREDICTIONS (Potential Edge Cases)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "least_confident = df.nsmallest(5, 'confidence_score')\n",
    "for idx, (_, row) in enumerate(least_confident.iterrows(), 1):\n",
    "    text = row[text_column][:100] + \"...\" if len(row[text_column]) > 100 else row[text_column]\n",
    "    print(f\"{idx}. Text: {text}\")\n",
    "    print(f\"   Predicted: {row['predicted_sentiment']} (confidence: {row['confidence_score']:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y7z8a9b0",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "output_file = \"sentiment_analysis_results.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_file = \"sentiment_summary.txt\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"SENTIMENT ANALYSIS SUMMARY REPORT\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Model: {MODEL_NAME}\\n\")\n",
    "    f.write(f\"Total texts analyzed: {len(df)}\\n\\n\")\n",
    "    f.write(\"Sentiment Distribution:\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    for sentiment in sentiment_counts.index:\n",
    "        count = sentiment_counts[sentiment]\n",
    "        pct = sentiment_percentages[sentiment]\n",
    "        f.write(f\"  {sentiment}: {count} ({pct:.2f}%)\\n\")\n",
    "    f.write(\"\\nConfidence Statistics:\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(f\"  Mean: {df['confidence_score'].mean():.4f}\\n\")\n",
    "    f.write(f\"  Median: {df['confidence_score'].median():.4f}\\n\")\n",
    "    f.write(f\"  Std Dev: {df['confidence_score'].std():.4f}\\n\")\n",
    "\n",
    "print(f\"Summary report saved to {summary_file}\")\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
